{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a3a7410",
   "metadata": {},
   "source": [
    "# Bench Marking Models on 3 Custom Datasets\n",
    "## Info on Dataset\n",
    "### Custom Dataset 1\n",
    "* Train - 8378 images\n",
    "* Test - 65 images\n",
    "* Val - 1505 images\n",
    "### Custom Dataset 2\n",
    "* Train - 10359 images\n",
    "* Test - 1470 images\n",
    "* Val - 2922 images\n",
    "### Custom Dataset 3\n",
    "* Train - 9939 images\n",
    "* Test - 474 images\n",
    "* Val - 947 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6df03d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH_1 = r\"C:\\Users\\teama\\Computer vision\\app-drone-detector-prototype\\my-app-local\\models\\1_best.pt\"\n",
    "MODEL_PATH_2 = r\"C:\\Users\\teama\\Computer vision\\app-drone-detector-prototype\\my-app-local\\models\\2_best.pt\"\n",
    "# MODEL_PATH_3 = r\"C:\\Users\\teama\\Computer vision\\app-drone-detector-prototype\\my-app-local\\models\\3_best.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c23e98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH_1 = r\"D:\\downloads\\merged.v2i.yolov11\\data.yaml\"\n",
    "DATA_PATH_2 = r\"D:\\second_run_on_yolo\\My First Project.v3i.yolov11\\data.yaml\"\n",
    "DATA_PATH_3 = r\"D:\\fourth_run_on_yolo\\data.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "024c5a36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'8.3.156'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ultralytics\n",
    "ultralytics.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc042de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "CUDA device: 0\n",
      "CUDA device name: NVIDIA GeForce RTX 3050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import torch\n",
    "import yaml\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA device:\", torch.cuda.current_device())\n",
    "print(\"CUDA device name:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "edafdf71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 'D:\\\\fourth_run_on_yolo\\\\train\\\\images',\n",
       " 'val': 'D:\\\\fourth_run_on_yolo\\\\valid\\\\images',\n",
       " 'test': 'D:\\\\fourth_run_on_yolo\\\\test\\\\images',\n",
       " 'nc': 5,\n",
       " 'names': {0: '-',\n",
       "  1: '- collaborate with your team on computer vision projects',\n",
       "  2: 'Drone_detection - v3 2024-02-22 2-29pm',\n",
       "  3: 'Roboflow is an end-to-end computer vision platform that helps you',\n",
       "  4: 'This dataset was exported via roboflow.com on March 1- 2024 at 8-22 AM GMT'},\n",
       " 'roboflow': {'workspace': 'traindronyolov9',\n",
       "  'project': 'deon_4_broject',\n",
       "  'version': 3,\n",
       "  'license': 'CC BY 4.0',\n",
       "  'url': 'https://universe.roboflow.com/traindronyolov9/deon_4_broject/dataset/3'},\n",
       " 'yaml_file': 'D:\\\\fourth_run_on_yolo\\\\data.yaml',\n",
       " 'channels': 3,\n",
       " 'path': WindowsPath('D:/fourth_run_on_yolo')}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ultralytics.data.utils import check_det_dataset\n",
    "\n",
    "check_det_dataset(DATA_PATH_1)\n",
    "check_det_dataset(DATA_PATH_2)\n",
    "check_det_dataset(DATA_PATH_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74f7bf06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.156  Python-3.10.18 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 3050 Laptop GPU, 4096MiB)\n",
      "YOLO11m summary (fused): 125 layers, 20,030,803 parameters, 0 gradients, 67.6 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.60.0 ms, read: 50.412.9 MB/s, size: 37.4 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning D:\\downloads\\merged.v2i.yolov11\\test\\labels.cache... 65 images, 0 backgrounds, 0 corrupt: 100%|██████████| 65/65 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:02<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         65         65      0.912      0.793      0.896      0.685\n",
      "Speed: 1.3ms preprocess, 26.7ms inference, 0.0ms loss, 4.3ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val\u001b[0m\n",
      "Ultralytics 8.3.156  Python-3.10.18 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 3050 Laptop GPU, 4096MiB)\n",
      "YOLO11m summary (fused): 125 layers, 20,030,803 parameters, 0 gradients, 67.6 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 543.4298.2 MB/s, size: 49.1 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning D:\\downloads\\merged.v2i.yolov11\\test\\labels.cache... 65 images, 0 backgrounds, 0 corrupt: 100%|██████████| 65/65 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:05<00:00,  1.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         65         65      0.968      0.938      0.982      0.739\n",
      "Speed: 1.9ms preprocess, 29.2ms inference, 0.0ms loss, 1.3ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val2\u001b[0m\n",
      "\n",
      "Test Set Metrics of 3 models on Custom Dataset 1:\n",
      "mAP@0.5 of 1_best.pt: 0.8956387305684872\n",
      "mAP@0.5:0.95 of 1_best: 0.6850080995312211\n",
      "mAP@0.5 of 2_best.pt: 0.9824268524632619\n",
      "mAP@0.5:0.95 of 2_best.pt: 0.7388705263541737\n"
     ]
    }
   ],
   "source": [
    "# TEST SET METRICS(mAP@0.5 & mAP@0.5:0.95)OF DIFFERENT MODELS ON CUSTOM DATASET 1\n",
    "model_1 = YOLO(MODEL_PATH_1)\n",
    "model_2 = YOLO(MODEL_PATH_2)\n",
    "# model_3 = YOLO(MODEL_PATH_3)\n",
    "\n",
    "model_1_metrics = model_1.val(data=DATA_PATH_1, split='test')\n",
    "model_2_metrics = model_2.val(data=DATA_PATH_1, split='test')\n",
    "# model_3_metrics = model_3.val(data=DATA_PATH_1, split='test')\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nTest Set Metrics of 3 models on Custom Dataset 1:\")\n",
    "print(\"mAP@0.5 of 1_best.pt:\", model_1_metrics.box.map50)\n",
    "print(\"mAP@0.5:0.95 of 1_best:\", model_1_metrics.box.map)\n",
    "\n",
    "\n",
    "print(\"mAP@0.5 of 2_best.pt:\", model_2_metrics.box.map50)\n",
    "print(\"mAP@0.5:0.95 of 2_best.pt:\", model_2_metrics.box.map)\n",
    "\n",
    "# print(\"mAP@0.5 of 3_best.pt:\", model_3_metrics.box.map50)\n",
    "# print(\"mAP@0.5:0.95 of 3_best.pt:\", model_3_metrics.box.map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5c1309c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.156  Python-3.10.18 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 3050 Laptop GPU, 4096MiB)\n",
      "YOLO11m summary (fused): 125 layers, 20,030,803 parameters, 0 gradients, 67.6 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.30.1 ms, read: 122.680.9 MB/s, size: 63.6 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning D:\\second_run_on_yolo\\My First Project.v3i.yolov11\\test\\labels.cache... 1470 images, 0 backgrounds, 0 corrupt: 100%|██████████| 1470/1470 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 92/92 [00:36<00:00,  2.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1470       1537       0.89      0.759      0.817        0.4\n",
      "Speed: 0.2ms preprocess, 20.2ms inference, 0.0ms loss, 1.1ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val3\u001b[0m\n",
      "Ultralytics 8.3.156  Python-3.10.18 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 3050 Laptop GPU, 4096MiB)\n",
      "YOLO11m summary (fused): 125 layers, 20,030,803 parameters, 0 gradients, 67.6 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.1 ms, read: 559.2358.7 MB/s, size: 69.9 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning D:\\second_run_on_yolo\\My First Project.v3i.yolov11\\test\\labels.cache... 1470 images, 0 backgrounds, 0 corrupt: 100%|██████████| 1470/1470 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 92/92 [00:36<00:00,  2.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1470       1537      0.963      0.909      0.957      0.644\n",
      "Speed: 0.2ms preprocess, 20.2ms inference, 0.0ms loss, 1.2ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val4\u001b[0m\n",
      "\n",
      "Test Set Metrics of 3 models on Custom Dataset 2:\n",
      "mAP@0.5 of 1_best.pt: 0.8171377373590349\n",
      "mAP@0.5:0.95 of 1_best: 0.39992766519910083\n",
      "mAP@0.5 of 2_best.pt: 0.9572764352475542\n",
      "mAP@0.5:0.95 of 2_best.pt: 0.6437786043025778\n"
     ]
    }
   ],
   "source": [
    "# TEST SET METRICS(mAP@0.5 & mAP@0.5:0.95)OF DIFFERENT MODELS ON CUSTOM DATASET 2\n",
    "model_1 = YOLO(MODEL_PATH_1)\n",
    "model_2 = YOLO(MODEL_PATH_2)\n",
    "# model_3 = YOLO(MODEL_PATH_3)\n",
    "\n",
    "model_1_metrics = model_1.val(data=DATA_PATH_2, split='test')\n",
    "model_2_metrics = model_2.val(data=DATA_PATH_2, split='test')\n",
    "# model_3_metrics = model_3.val(data=DATA_PATH_2, split='test')\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nTest Set Metrics of 3 models on Custom Dataset 2:\")\n",
    "print(\"mAP@0.5 of 1_best.pt:\", model_1_metrics.box.map50)\n",
    "print(\"mAP@0.5:0.95 of 1_best:\", model_1_metrics.box.map)\n",
    "\n",
    "\n",
    "print(\"mAP@0.5 of 2_best.pt:\", model_2_metrics.box.map50)\n",
    "print(\"mAP@0.5:0.95 of 2_best.pt:\", model_2_metrics.box.map)\n",
    "\n",
    "# print(\"mAP@0.5 of 3_best.pt:\", model_3_metrics.box.map50)\n",
    "# print(\"mAP@0.5:0.95 of 3_best.pt:\", model_3_metrics.box.map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "531b8e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.156  Python-3.10.18 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 3050 Laptop GPU, 4096MiB)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 184.6221.7 MB/s, size: 36.3 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning D:\\fourth_run_on_yolo\\test\\labels.cache... 1376 images, 0 backgrounds, 0 corrupt: 100%|██████████| 1376/1376 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):   0%|          | 0/86 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 4 is out of bounds for axis 1 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# TEST SET METRICS(mAP@0.5 & mAP@0.5:0.95)OF DIFFERENT MODELS ON CUSTOM DATASET 3\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# model_1 = YOLO(MODEL_PATH_1)\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# model_2 = YOLO(MODEL_PATH_2)\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# model_3 = YOLO(MODEL_PATH_3)\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m model_1_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDATA_PATH_3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m model_2_metrics \u001b[38;5;241m=\u001b[39m model_2\u001b[38;5;241m.\u001b[39mval(data\u001b[38;5;241m=\u001b[39mDATA_PATH_3, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# model_3_metrics = model_3.val(data=DATA_PATH_3, split='test')\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\teama\\miniconda3\\envs\\dl-env-ab\\lib\\site-packages\\ultralytics\\engine\\model.py:633\u001b[0m, in \u001b[0;36mModel.val\u001b[1;34m(self, validator, **kwargs)\u001b[0m\n\u001b[0;32m    630\u001b[0m args \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moverrides, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcustom, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m}  \u001b[38;5;66;03m# highest priority args on the right\u001b[39;00m\n\u001b[0;32m    632\u001b[0m validator \u001b[38;5;241m=\u001b[39m (validator \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_smart_load(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidator\u001b[39m\u001b[38;5;124m\"\u001b[39m))(args\u001b[38;5;241m=\u001b[39margs, _callbacks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks)\n\u001b[1;32m--> 633\u001b[0m \u001b[43mvalidator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics \u001b[38;5;241m=\u001b[39m validator\u001b[38;5;241m.\u001b[39mmetrics\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m validator\u001b[38;5;241m.\u001b[39mmetrics\n",
      "File \u001b[1;32mc:\\Users\\teama\\miniconda3\\envs\\dl-env-ab\\lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\teama\\miniconda3\\envs\\dl-env-ab\\lib\\site-packages\\ultralytics\\engine\\validator.py:221\u001b[0m, in \u001b[0;36mBaseValidator.__call__\u001b[1;34m(self, trainer, model)\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m dt[\u001b[38;5;241m3\u001b[39m]:\n\u001b[0;32m    219\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(preds)\n\u001b[1;32m--> 221\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mplots \u001b[38;5;129;01mand\u001b[39;00m batch_i \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplot_val_samples(batch, batch_i)\n",
      "File \u001b[1;32mc:\\Users\\teama\\miniconda3\\envs\\dl-env-ab\\lib\\site-packages\\ultralytics\\models\\yolo\\detect\\val.py:201\u001b[0m, in \u001b[0;36mDetectionValidator.update_metrics\u001b[1;34m(self, preds, batch)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mplots:\n\u001b[1;32m--> 201\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfusion_matrix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m no_pred:\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\teama\\miniconda3\\envs\\dl-env-ab\\lib\\site-packages\\ultralytics\\utils\\metrics.py:402\u001b[0m, in \u001b[0;36mConfusionMatrix.process_batch\u001b[1;34m(self, detections, batch, conf, iou_thres)\u001b[0m\n\u001b[0;32m    400\u001b[0m j \u001b[38;5;241m=\u001b[39m m0 \u001b[38;5;241m==\u001b[39m i\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28msum\u001b[39m(j) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 402\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmatrix[detection_classes[m1[j]\u001b[38;5;241m.\u001b[39mitem()], gc] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# correct\u001b[39;00m\n\u001b[0;32m    403\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmatrix[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnc, gc] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# true background\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 4 is out of bounds for axis 1 with size 2"
     ]
    }
   ],
   "source": [
    "# TEST SET METRICS(mAP@0.5 & mAP@0.5:0.95)OF DIFFERENT MODELS ON CUSTOM DATASET 3\n",
    "# model_1 = YOLO(MODEL_PATH_1)\n",
    "# model_2 = YOLO(MODEL_PATH_2)\n",
    "# model_3 = YOLO(MODEL_PATH_3)\n",
    "\n",
    "model_1_metrics = model_1.val(data=DATA_PATH_3, split='test')\n",
    "model_2_metrics = model_2.val(data=DATA_PATH_3, split='test')\n",
    "# model_3_metrics = model_3.val(data=DATA_PATH_3, split='test')\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nTest Set Metrics of 3 models on Custom Dataset 2:\")\n",
    "print(\"mAP@0.5 of 1_best.pt:\", model_1_metrics.box.map50)\n",
    "print(\"mAP@0.5:0.95 of 1_best:\", model_1_metrics.box.map)\n",
    "\n",
    "\n",
    "print(\"mAP@0.5 of 2_best.pt:\", model_2_metrics.box.map50)\n",
    "print(\"mAP@0.5:0.95 of 2_best.pt:\", model_2_metrics.box.map)\n",
    "\n",
    "# print(\"mAP@0.5 of 3_best.pt:\", model_3_metrics.box.map50)\n",
    "# print(\"mAP@0.5:0.95 of 3_best.pt:\", model_3_metrics.box.map)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6c4f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "summary(model_1.model, input_size=(1, 3, 640, 640))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207403a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "summary(model_2.model, input_size=(1, 3, 640, 640))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e539b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "summary(model_3.model, input_size=(1, 3, 640, 640))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-env-ab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
